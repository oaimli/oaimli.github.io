---
layout: post
title: 贝叶斯决策理论
key: 2018011102
tags: MachineLearning
mathjax: true
modify_date: 2018-01-12
---

>转载请标明出处：  
>https://seektech.github.io/2018/01/11/贝叶斯决策理论.html [**Miao LI (seektech)**](https://seektech.github.io/2018/01/11/贝叶斯决策理论.html)

贝叶斯决策理论研究了模式类的的概率结构完全知道的情况。

贝叶斯决策理论是解决模式分类问题的一种基本统计途径，假设了决策问题可以用概率的形式来描述并且假设所有有关的概率结构均已知。

条件概率公式：

$$\displaystyle p(x|y) = \frac {p(xy)} {p(y)} $$

贝叶斯公式：

$$\displaystyle p(B|A)=\frac {p(A|B)p(B)}{p(A)}$$

全概率公式：

$$p(A)=\sum \limits_{i=1}^n p(B_i)p(A|B_i)$$

**问题表示：**

类别-$\displaystyle \omega_i, i=1,…,c$

特征矢量-$\mathbf{x}=[x_1,\dots,x_d] \in \mathbb{R}^d$

先验概率-$p(\omega_i)$  $\sum \limits_{i=1}^c p(\omega_i)=1$

概率密度函数(条件概率)-$p(\mathbf{x} \mid \omega_i)$

后验概率-$\displaystyle p(\omega_i \mid \mathbf{x})=\frac {p(\mathbf{x} \mid \omega_i)p(\omega_i)}{p(\mathbf{x})}=\frac {p(\mathbf{x} \mid \omega_i)p(\omega_i)}{\sum \limits_{j=1}^c p(\mathbf{x} \mid \omega_j)p(\omega_j)}$

### [](#header-1)一、最小错误率决策

> 决策过程中自然要寻找使得决策错误率最小的决策行为

**1.** 当只知道先验概率

$$\displaystyle p(error) = \left \lbrace \begin{array} {c} p(\omega_2) \qquad if \ decide \ \omega_1 \\ p(\omega_1) \qquad if \ decide \ \omega_2 \end{array} \right.$$

最小错误率决策，即如果$p(\omega_1) >p(\omega_2)$，决策$\omega_1$，否则$\omega_2$

**2.** 基于后验概率决策

$$\displaystyle p(error) = \left \lbrace \begin{array} {c} p(\omega_2 | \mathbf{x}) \qquad if \ decide \ \omega_1 \\ p(\omega_1| \mathbf{x}) \qquad if \ decide \ \omega_2 \end{array} \right.$$

最小错误率决策，即最大后验概率决策，如果$p(\omega_1 \mid \mathbf{x}) >p(\omega_2 \mid \mathbf{x})$，决策$\omega_1$，否则$\omega_2$

根据后验搞率公式，如果$p(\mathbf{x} \mid \omega_1)p(\omega_1) >p(\mathbf{x}\mid \omega_1)p(\omega_2)$，决策$\omega_1$，否则$\omega_2$


### [](#header-2)二、最小风险决策（贝叶斯决策）

**1.** 最小风险决策

决策代价：将正确的类别$\omega_j$决策为$\alpha_i$时代价(loss)为 $\lambda_{ij}=\lambda(\alpha_i \mid \omega_j)$

条件风险(Conditional Risk)：$\displaystyle \mathrm{R}(\alpha_i \mid \mathbf{x})=\sum \limits_{j=1}^c \lambda(\alpha_i \mid \omega_j)p(\omega_j \mid \mathbf{x}) $

期望风险：$\displaystyle \mathrm{R}=\int \nolimits \mathrm{R}(\alpha(\mathbf{x})\mid \mathbf{x}) p(\mathbf{x})d \mathbf{x}$

贝叶斯决策：$\displaystyle \arg \underset{i}{\min} \mathrm{R}(\alpha_i \mid \mathrm{x})$

**2-class case：**

$$\displaystyle \mathrm{R}(\alpha_1 \mid \mathbf{x}) = \lambda_{11} p(\omega_1 \mid \mathbf{x})+\lambda_{12} p(\omega_2 \mid \mathbf{x})$$

$$\displaystyle \mathrm{R}(\alpha_2 \mid \mathbf{x}) = \lambda_{21} p(\omega_1 \mid \mathbf{x})+\lambda_{22} p(\omega_2 \mid \mathbf{x})$$

决策$\omega_1$，如果$\mathrm{R}(\alpha_1 \mid \mathbf{x}) <\mathrm{R}(\alpha_2 \mid \mathbf{x})$，否则$\omega_1$

即，

$$\displaystyle \begin{align*} \lambda_{11} p(\omega_1 \mid \mathbf{x})+\lambda_{12} p(\omega_2 \mid \mathbf{x}) &< \lambda_{21} p(\omega_1 \mid \mathbf{x})+\lambda_{22} p(\omega_2 \mid \mathbf{x}) \\ \displaystyle (\lambda_{21}-\lambda_{11})p(\mathbf{x} | \omega_1)p(\omega_1) & >(\lambda_{12}-\lambda_{22})p(\mathbf{x} | \omega_2)p(\omega_2)\\ \displaystyle \frac {p(\mathbf{x} | \omega_1)}{p(\mathbf{x} | \omega_2)} &> \frac {(\lambda_{12}-\lambda_{22})p(\omega_2)}{(\lambda_{21}-\lambda_{11})p(\omega_1) } \end{align*} $$

当决策代价为01损失时，

$$\displaystyle \lambda(\alpha_i \mid \omega_j) = \left \lbrace \begin{array}{c} 0 \qquad  i=j \\ 1 \qquad i \neq j\end{array} \right. \qquad i,j = 1, \dots ,c$$

$$\displaystyle \begin{align*} \mathrm{R}(\alpha_i \mid \mathbf{x}) & =\sum \limits_{j=1}^c \lambda(\alpha_i \mid \omega_j)p(\omega_j \mid \mathbf{x}) \\ & = \sum \limits_{j \neq i}p(\omega_j | \mathbf{x}) \\ & = 1-p(\omega_i | \mathbf{x})\end{align*}$$

最小风险决策，决策$\omega_i$，如果$p(\omega_i  \mid \mathbf{x}) > p(\omega_j \mid \mathbf{x})$，这时的最小风险决策就等价于最大后验概率决策

**2.**带拒识的决策

在必要情况下，分类器对于某些样本可以拒绝给出一个输出结果（后面可以转交给人工处理）

在引入拒识（reject）的情况下，分类器可以拒绝将样本判为 c 个类别中的任何一类

此时的损失定义如下：

$$\lambda_{ij}=\begin{cases} 0 & i=j;\\ \lambda_s & i\not=j; \\ \lambda_r\quad(\lambda_r<\lambda_s) & reject.\end{cases}$$

拒识决策的代价须小于错分代价，否则就永远都不会对样本拒识了

在这种情况下，条件风险的表达式为：

所以带拒识的决策的最小风险决策为：

**3.** 贝叶斯决策用于模式分类 

分类过程：

- 计算样本属于每一类的后验概率
- 最大后验概率/最小风险决策

分类器设计：

- 收集训练样本
- 用每一类的样本估计类条件概率密度$p(\mathbf{x} \mid \omega_i)$
- 估计类先验概率


### [](#header-3)三、判别函数与决策面

**1.** 多类情况下的判别函数

有很多种 方式来表述模式分类器，使用最多的是判别函数$g_i(\mathbf{x}), i=1, \dots ,c$，如果对于所有的$j \neq i$，有$g_i(\mathbf{x}>g_j(\mathbf{x}))$，则分类器将此特征向量$\mathbf{x}$判决为$\omega_i$

与最小条件风险对应：

$$g_i(\mathbf{x})=-\mathrm{R}(\alpha_i | \mathbf{x})$$

与最大后验概率对应：

$$g_i(\mathbf{x})=p(\omega_i | \mathbf{x}) \ or \ g_i(\mathbf{x})=p(\mathbf{x} | \omega_i)p(\omega_i)$$

取对数便于计算和分析：

$$g_i(\mathbf{x})=\ln p(\mathbf{x} | \omega_i) + \ln p(\omega_i)$$

**2.** 两类情况下的判别函数

二类分类器，并非使用两个判别函数

定义一个简单的判别函数$g(\mathbf{x}) = g_1(\mathbf{x})-g_2(\mathbf{x})$，如果$g(\mathbf{x})>0$，则判为$\omega_1$，否则判为$\omega_2$

$$g(\mathbf{x})=p(\omega_1 | \mathbf{x})-p(\omega_2 | \mathbf{x})$$

$$\displaystyle g(\mathbf{x})=\ln \frac{p(\omega_1 | \mathbf{x})}{p(\omega_2 | \mathbf{x})} = \ln \frac {p(\mathbf{x} | \omega_1)}{p(\mathbf{x} | \omega_2)} + \ln \frac {p(\omega_1)}{p(\omega_2)}$$

### [](#header-4)四、高斯密度函数

**1.** 单变量密度函数 $p(x) \sim \mathcal{N}(\mu , \sigma^2)$

$$\displaystyle p(x) = \frac {1}{\sqrt{2 \pi} \sigma} \exp \left [ -\frac{1}{2} \left( \frac{x-\mu}{\sigma}\right)^2\right]$$

$x$的期望值：

$$\displaystyle \mu \equiv \mathcal{E} [x] = \int_{-\infty}^\infty xp(x)dx$$

$x$的方差：

$$\displaystyle \sigma^2 = \mathcal{E}[(x-\mu)^2] = \int_{- \infty}^\infty(x-\mu)^2p(x)dx$$

![](https://ws3.sinaimg.cn/large/006tNc79ly1fnfy0ne7lxj30b107v74x.jpg)

单变量正态分布完全由两个参数决定，$p(x) \sim \mathcal{N}(\mu , \sigma^2)$，服从正态分布的样本聚集于均值附近，其散布程度与标准差有关，单变量正态分布大约有95%的区域在$\mid x-\mu \mid \leq 2\sigma$范围内，峰值为$p(\mu) = 1 / \sqrt{2 \pi} \sigma$

可以证明正态分布在所有具有给定的均值和方差的分布中具有最大熵：

（证明）

根据中心极限定理，大量独立随机变量之和趋于正态分布，实际环境中很多类别的特征分布趋于正态分布

**2.** 多元密度函数 $p(\mathbf{x}) \sim \mathcal{N}(\pmb\mu , \pmb\Sigma)$

一般的$d$维多元正态密度的形式如下，其中$\mathbf{x}$是一个$d$维列向量，$\pmb \mu$是$d$维均值向量，$\pmb \Sigma$	是$d \times d$的协方差矩阵，$\mid \pmb \Sigma \mid$和$\pmb \Sigma^{-1}$分别是协方差矩阵的行列式值和逆

$$\displaystyle p(\mathbf{x}) = \frac {1}{(2\pi)^{d/2}|\pmb \Sigma|^{1/2}} \exp \left[ -\frac{1}{2}(\mathbf{x}-\pmb \mu)^t \pmb \Sigma ^{-1}(\mathbf{x}-\pmb \mu)\right]$$

$$\displaystyle \pmb\mu \equiv \mathcal{E}[\mathbf{x}] = \int \mathbf{x}p(\mathbf{x})d\mathbf{x}$$

$$\displaystyle \pmb\Sigma \equiv \mathcal{E}[(\mathbf{x}-\pmb\mu)(\mathbf{x}-\pmb\mu)^t]=\int(\mathbf{x}-\pmb\mu)(\mathbf{x}-\pmb\mu)^tp(\mathbf{x})d\mathbf{x}$$

其中，如果$x_i$是$\mathbf{x}$的第$i$个元素，$\mu_i$是$\pmb \mu$的第$i$个元素，$\sigma_{ij}$是$\pmb \Sigma$的第$ij$个元素($x_i$和$x_j$统计独立，则$\sigma_{ij}=0$)，$\pmb \Sigma$的对角元素$\sigma_{ii}$是相应的$x_i$的方差，非对角元素$\sigma_{ij}$是相应的$x_i$和$x_j$的协方差

$$\mu_i = \mathcal{E}[x_i]$$

$$\sigma_{ij} = \mathcal{E}[(x_i-\mu_i)(x_j-\mu_j)]$$

**3.** 协方差矩阵$\pmb \Sigma$的性质

协方差矩阵的Eigenvalues & eigenvecters：

$$\pmb \Sigma \phi_i = \lambda_i \phi_i$$(特征值定义)

$$\pmb \Phi = [\phi_1\phi_2 \cdots\phi_d]$$

$$\pmb \Lambda = diag[\lambda_1\lambda_2\cdots\lambda_d]$$

$\pmb \Phi$为正交阵，$\pmb \Phi^t \pmb \Phi = \pmb I$，$\pmb \Phi^{-1}=\pmb \Phi^t$，

$$\Phi_i^t\Phi_j =\left \lbrace \begin{array}{c}  1, \ i=j\\ 0, \ i \neq j \end{array}\right.$$

矩阵表示：

$$\pmb \Sigma \pmb \Phi = \pmb\Phi \pmb \Lambda$$

$$\pmb \Sigma  = \pmb\Phi \pmb \Lambda\pmb \Phi^T= \sum \limits_{i=1}^d \lambda_i \Phi_i \Phi_i^T$$

矩阵对角化：

$$\pmb\Phi^T \pmb \Sigma\pmb \Phi = \Lambda$$

$$\pmb\Phi^T_i \pmb \Sigma \pmb \Phi_i = \lambda_i$$

应用-PCA(Principal Component Analysis )：

（PCA的推导）



**4.** 线性变换的高斯分布

服从正态分布的随机变量的线性组合，不管这些随机变量是独立的还是非独立的，也是一个正态分布

如果$p(\mathbf{x}) \sim \mathcal{N}(\pmb\mu , \pmb\Sigma)$，$\mathbf{A}$是一个$d \times k$的矩阵，$\mathbf{y}=\mathbf{A}^t\mathbf{x}$是一个$k$维向量，那么$p(\mathbf{y}) \sim \mathcal{N}(\mathbf{A}^t\pmb\mu , \mathbf{A}^t\pmb\Sigma \mathbf{A})$

在$k=1$且$\mathbf{A}$是一单位向量$\mathbf{a}$的情况下，$y=\mathbf{A}^t\mathbf{x}$是一标量，表示$\mathbf{x}$到$\mathbf{a}$方向的一条直线的投影，$\mathbf{a}^t \pmb\Sigma \mathbf{a}$是$\mathbf{x}$向$\mathbf{a}$投影的方差，产生了沿该直线方向的$\mathcal{N}(\mu , \sigma^2)$，因此可以计算数据验任何方向或自空间的分散程度

![](https://ws4.sinaimg.cn/large/006tNc79ly1fng74lfqz3j30b10b1di0.jpg)

如果定义矩阵$\pmb \Phi$，列向量是$\pmb \Sigma$的正交本征向量，$\pmb \Lambda$为相应本征值对应的对角矩阵，那么变换$\mathbf{A}_\omega=\pmb \Phi \pmb \Lambda^{-1/2}$将使变换后的分布的协方差矩阵成为单位阵，

$$\mathbf{A}_\omega^t \pmb \Sigma \mathbf{A}_\omega = \pmb \Lambda^{-1/2}\pmb \Phi^t \pmb \Sigma\pmb \Phi\pmb \Lambda^{-1/2} = \pmb \Lambda^{-1/2}\pmb \Lambda\pmb \Lambda^{-1/2} = \mathbf{I}$$

变换后的分布的本征向量谱具有均匀性，因此$A_\omega$被称为白化变换ZCA(Zero Component Analysis)

**5.** 白化变换ZCA推导

(ZCA推导)

**6.** 相关知识

- Dimensionality reduction降维


- Feature extraction特征提取
  - Feature generation: original data—>$\mathbf{x}$
  - Linear feature extraction,$\mathbf{x} = \mathbf{A}^T\mathbf{d}$
- Feature selection (for reduction and performance)特征选择
  - Feature subset selection: a learning/optimization problem
- Feature transform (for extraction or reduction)特征变换
  - Linear transform, $\mathbf{y} = \mathbf{A}^T\mathbf{x}$
  - Nonlinear transform: may increase dimensionality, e.g. kernel PCA,kernel LDA
- Handcrafted feature手工特征
- Feature learning特征学习
  - Automatic feature generation, e.g. convolutional neural network (CNN)

### [](#header-5)五、高斯密度下的判别函数

$$\displaystyle p(\mathbf{x} | \omega_i) = \frac {1}{(2\pi)^{d/2}|\pmb \Sigma_i|^{1/2}} \exp \left[ -\frac{1}{2}(\mathbf{x}-\pmb \mu_i)^t \pmb \Sigma ^{-1}(\mathbf{x}-\pmb \mu_i)\right]$$

判别函数：

$$\displaystyle g_i(\mathbf{x})=\ln p(\mathbf{x} | \omega_i) + \ln p(\omega_i) = - \frac{1}{2}(\mathbf{x}-\pmb \mu_i)^t\pmb \Sigma_i^{-1}(\mathbf{x}-\pmb \mu_i)-\frac{d}{2} \ln 2\pi-\frac{1}{2} \ln|\pmb \Sigma_i|+\ln p(\omega_i)$$

**情况1:**$\pmb \Sigma_i = \pmb \sigma^2 \mathbf{I}$

特征统计独立，每个特征具有相同的方差$\sigma^2$，协方差矩阵为对角阵，$\mid \pmb \Sigma_i \mid=\sigma^{2d}$，$\pmb \Sigma_i^{-1}=\frac{1}{\sigma^2} \mathbf{I}$

几何上，与样本落于相等大小的超球体聚类中的情况相对应，第$i​$类的聚类以均值向量$\pmb \mu_i​$为中心

化简掉与$i$无关的项，判别函数:

$$\displaystyle g_i(\mathbf{x}) = - \frac{\|\mathbf{x}-\pmb \mu_i \|^2}{2 \sigma^2}+\ln p(\omega_i)$$

$$\displaystyle \|\mathbf{x}-\pmb \mu_i \|^2 = (\mathbf{x}-\pmb \mu_i)^t(\mathbf{x}-\pmb \mu_i)$$

展开二次式$(\mathbf{x}-\pmb \mu_i)^t(\mathbf{x}-\pmb \mu_i)$，判别函数为：

$$\displaystyle g_i(\mathbf{x}) = - \frac{1}{2 \sigma^2}\left [ \mathbf{x}^t\mathbf{x}-2\pmb \mu_i^t\mathbf{x}+\pmb\mu_i^t\pmb\mu_i\right ]+\ln p(\omega_i)$$

其中，$\mathbf{x}^t\mathbf{x}$对所有$i$相等，得到等价的线性判别函数：

$$g_i(\mathbf{x}) = \mathbf{w}_i^t \mathbf{x}+\omega_{i0}$$

$$\mathbf{w}_i = \frac{1}{\sigma^2}\pmb \mu_i$$

$$\displaystyle \omega_{i0} = -\frac{1}{2\sigma^2}\pmb\mu_i^t\pmb\mu_i+\ln p(\omega_i)$$

$\omega_{i0}$为第$i$个方向的阈值或偏置

二类决策面由判别函数相等$g_i(\mathbf{x})-g_j(\mathbf{x})=0$的点构成：

$$\mathbf{w}^t(\mathbf{x}-\mathbf{x}_0) = 0$$

其中，

$$\mathbf{w} = \pmb\mu_i-\pmb\mu_j$$

$$\displaystyle \mathbf{x}_0 = \frac{1}{2}(\pmb\mu_i+\pmb\mu_j)-\frac{\sigma^2}{\|\pmb\mu_i-\pmb\mu_j\|^2}\ln \frac{p(\omega_i)}{p(\omega_j)}(\pmb\mu_i-\pmb\mu_j)$$

此方程定义了一个通过点$\mathbf{x}_0$且与向量$\mathbf{w}$正交的超平面，由于$\mathbf{w} = \pmb\mu_i-\pmb\mu_j$，将$\mathcal{R}_i$与$\mathcal{R}_j$分开的超平面与两中心点的连线垂直

如果$p(\omega_i)=p(\omega_j)$，$\mathbf{x}_0$第二项为0，因此点$\mathbf{x}_0$位于两中心的中点，且超平面垂直平分两中心的连线

如果$p(\omega_i) \neq p(\omega_j)$，$\mathbf{x}_0$将远离可能的均值

如果$\sigma^2$相对于平方距离$\|\pmb\mu_i-\pmb\mu_j\|^2$较小，即$\mathbf{x}_0$第二项较小，那么判决边界的位置相对于确切的先验概率值并不敏感

**情况2:**$\pmb \Sigma_i = \pmb \Sigma$    **LDF**

所有类的协方差矩阵都相等，但各自的均值向量是任意的

几何上，与样本落于相等大小和相同形状的超椭球体聚类中的情况相对应，第$i$类的聚类中心在$\pmb \mu_i$附近

判别函数化简为：

$$\displaystyle g_i(\mathbf{x})= - \frac{1}{2}(\mathbf{x}-\pmb \mu_i)^t\pmb \Sigma^{-1}(\mathbf{x}-\pmb \mu_i)+\ln p(\omega_i)$$

如果先验概率$p(\omega_i)$都相等，那么$\ln p(\omega_i)$可以被忽略，则判决规则简化为：计算从$\mathbf{x}$到每一个$c$均值向量的平方马氏距离，将$\mathbf{x}$归于离它最近的均值所属的类

不相等的先验概率将判定面移向远离先验概率大的一边

展开二次式$(\mathbf{x}-\pmb \mu_i)^t\pmb\Sigma^{-1}(\mathbf{x}-\pmb \mu_i)$，其中，$\mathbf{x}^t\pmb\Sigma^{-1}\mathbf{x}$对所有$i$相等，得到等价的线性判别函数：

$$g_i(\mathbf{x}) = \mathbf{w}_i^t \mathbf{x}+\omega_{i0}$$

$$\mathbf{w}_i = \pmb \Sigma^{-1} \pmb \mu_i$$

$$\displaystyle \omega_{i0} = -\frac{1}{2}\pmb\mu_i^t \pmb \Sigma^{-1}\pmb\mu_i+\ln p(\omega_i)$$

二类决策面依然是超平面：

$$\mathbf{w}^t(\mathbf{x}-\mathbf{x}_0) = 0$$

其中，

$$\mathbf{w} = \pmb\Sigma^{-1}(\pmb\mu_i-\pmb\mu_j)$$

$$\displaystyle \mathbf{x}_0 = \frac{1}{2}(\pmb\mu_i+\pmb\mu_j)-\frac{1}{(\pmb\mu_i-\pmb\mu_j)^t\pmb\Sigma^{-1}(\mathbf{x}-\pmb\mu_j)}\ln \frac{p(\omega_i)}{p(\omega_j)}(\pmb\mu_i-\pmb\mu_j)$$

这时的$\mathbf{w}$并非朝着$\pmb\mu_i-\pmb\mu_j$的方向，分离$\mathcal{R}_i$与$\mathcal{R}_j$的超平面并非与两均值的连线垂直

如果$p(\omega_i)=p(\omega_j)$，判定面与均值连线交于$\mathbf{x}_0$处

如果$p(\omega_i) \neq p(\omega_j)$，分离超平面将远离可能的均值

**情况3:**$\pmb \Sigma_i = 任意$    **QDF**

每一类的协方差矩阵不同

约掉判别函数无关项，判别函数显然是二次型：

$$g_i(\mathbf{x}) = \mathbf{x}^t \mathbf{W} \mathbf{x}+\mathbf{w}_i^t \mathbf{x}+\omega_{i0}$$

$$\displaystyle \mathbf{W_i} = -\frac{1}{2}\pmb\Sigma_i^{-1}$$

$$\mathbf{w}_i = \pmb \Sigma_i^{-1} \pmb \mu_i$$

$$\displaystyle \omega_{i0} = -\frac{1}{2}\pmb\mu_i^t \pmb \Sigma_i^{-1}\pmb\mu_i-\frac{1}{2} \ln |\pmb\Sigma_i|+\ln p(\omega_i)$$

二类分类的判定面为超二次曲面

### [](#header-6)六、离散变量的贝叶斯决策

离散特征变量，概率密度函数$p(\mathbf{x} \mid \omega_i) = p(x_1x_2 \cdots x_d \mid \omega_i)$

**1.**贝叶斯决策

最小风险：$\min \mathbf{R}(\alpha_i \mid \mathbf{x}) = \sum \limits_{j=1}^c\lambda(\alpha_i \mid \omega_j)P(\omega_j \mid \mathbf{x})$

最小错误率(MAP)：$\max P(\omega_i \mid \mathbf{x})$

**2.** 独立二值特征

特征向量的元素是二值的:

$$p_i = Pr[x_i = 1 |\omega_1]$$

$$q_i = Pr[x_i = 1 |\omega_2]$$

并且条件独立:

$$p(\mathbf{x}) = p(x_1x_2\cdots x_d)=\prod \limits_{i=1}^dp(x_i)$$

因此类条件概率表示为：

$$P(\mathbf{x}|\omega_1) = \prod \limits_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}$$

$$P(\mathbf{x}|\omega_2) = \prod \limits_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}$$

那么似然比为：

$$\displaystyle \frac{P(\mathbf{x}|\omega_1)}{P(\mathbf{x}|\omega_2)} =  \prod \limits_{i=1}^d \left (\frac{p_i}{q_i}\right )^{x_i}\left (\frac{1-p_i}{1-q_i}\right )^{1-x_i}$$

根据两类情况的判决函数得判决函数为：

$$\displaystyle \begin{align}g(\mathbf{x})=&\ln \frac{p(\omega_1 | \mathbf{x})}{p(\omega_2 | \mathbf{x})} \\= &\ln \frac {p(\mathbf{x} | \omega_1)}{p(\mathbf{x} | \omega_2)} + \ln \frac {p(\omega_1)}{p(\omega_2)}\\ = &\sum \limits_{i=1}^d \left[ x_i \ln \frac{p_i}{q_i}+(1-x_i)\ln \frac{1-p_i}{1-q_i}\right] + \ln \frac {p(\omega_1)}{p(\omega_2)}\end{align}$$

判决函数对于$x_i$是线性的，可写成：

$$\displaystyle g(\mathbf{x})=\sum \limits_{i=1}^d \omega_ix_i + \omega_0$$

其中，

$$\displaystyle \omega_i = \ln \frac{p_i(1-q_i)}{q_i(1-p_i)}$$

$$\displaystyle \omega_0 = \sum \limits_{i=1}^d\ln \frac{1-p_i}{1-q_i}+\ln \frac {p(\omega_1)}{p(\omega_2)}$$

### [](#header-7)七、复合模式分类

**1.** 多个模式同时分类

$$\mathbf{X}=\mathbf{x}_1\mathbf{x}_2\cdots \mathbf{x}_n \qquad \pmb\omega =\omega(1)\omega(2)\cdots \omega(n)$$

此时贝叶斯决策，$\pmb \omega$类别数巨大，$p(\mathbf{X} \mid \pmb\omega)$存储和估计困难

条件独立：

$$\displaystyle p(\mathbf{X} \mid \pmb\omega) = \prod \limits_{i=1}^np(\mathbf{x}_i|\omega(i))$$

马尔可夫链：

$$\displaystyle p[\omega(1)\omega(2)\cdots \omega(n)] = p[\omega(1)]\prod \limits_{j=1}^np[\omega(j)|\omega(j-1)]$$

Hidden Markov model (HMM)是复合模式分类的代表模型之一

**2.** 复合模式分类用于多分类器融合

多个分类器的决策当作多维特征，Bayes方法重新分类

一个分类器的输出：离散变量$e_k \in \{\omega_1\omega_2\cdots\omega_M\}$

联合输出空间的后验概率：

$$\displaystyle P(\omega_i | e_1e_2\cdots e_K)  = \frac {P(e_1e_2\cdots e_K|\omega_i)P(\omega_i)}{P(e_1e_2\cdots e_K)} \quad i=1\dots M$$

数据集上估计离散空间的类条件概率密度$P(e_1e_2\cdots e_K \mid \omega_i)$，指数级复杂度，需要大量样本

Naive Bayes对类条件概率做出了条件独立性假设：

$$\displaystyle P(e_1=\omega_{j1}e_2=\omega_{j2}\cdots e_K=\omega_{jK}|\omega_i) = \prod \limits_{k=1}{K}(e_K=\omega_{jK}|\omega_i)$$

### [](#header-8)八、概率密度估计方法

参数法和非参数法都是对贝叶斯决策的近似，实际难以最优

- 参数法(见最大似然和贝叶斯估计)：假定概率密度函数形式，$p(\mathbf{x} \mid \omega_i) = p(\mathbf{x} \mid \theta_i)$

  -Distribution: Gaussian, Gamma, Bernouli

  -Estimation: Maximun-likelyhood, Bayesian Estimation

- 非参数法(见非参数方法)：可以表示任意概率分布

  -Parzen Window, K-NN

  -需要保存所有或者大部分样本

- Semi-Parametric

  -Distribution: Gaussian mexture

  -Estimation: Expectation-Maximization(EM)

该文章已同步至[CSDN](http://blog.csdn.net/u013413471/article/)  
