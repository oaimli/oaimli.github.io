---
layout: post
title: 非参数方法
key: 2018011801
tags: MachineLearning
mathjax: true
---

>转载请标明出处：  
>https://seektech.github.io/2018/01/18/非参数方法.html [**Miao LI (seektech)**](https://seektech.github.io/2018/01/18/非参数方法.html)


概率密度估计的方法：

- 参数法(见最大似然和贝叶斯估计)：假定概率密度函数形式，$p(\mathbf{x} \mid \omega_i) = p(\mathbf{x} \mid \theta_i)$

  -Distribution: Gaussian, Gamma, Bernouli

  -Estimation: Maximun-likelyhood, Bayesian Estimation

- **非参数法：可以表示任意概率分布**

  -Parzen Window

  -K-NN

- Semi-Parametric

  -Distribution: Gaussian mexture

  -Estimation: Expectation-Maximization(EM)

### [](#header-1)一、非参数技术的概率密度估计

在参数法对概率密度估计中，总是假设概率密度函数的参数形式已知，并在此条件下处理有监督学习过程，对于很多实际的模式识别应用场合，概率密度函数的参数形式不一定已知，而且一般给出了概率密度函数的参数形式很少复合实际情况

所有的经典的概率密度函数都是单模的，即只有单个局部极大值，但现实中遇到的常常是多模的情况，而且实际情况中关于高维密度可以表示为一维密度的乘积的假设也不一定成立

非参数方法能够处理任意的概率分布，不必假设密度的参数形式

**非参数方法：**

- 从训练样本估计概率密度函数$p(\mathbf{x} \mid \omega_j)$
- 直接估计后验概率$p( \omega_j \mid \mathbf{x} )$

估计概率密度函数的方法基于一个事实：一个向量$\mathbf{x}$落在区域$\mathcal{R}$中的概率为

$$\displaystyle P=\int \limits_{\mathcal{R}}p(\mathbf{x}')d\mathbf{x}'$$

因此可以通过估计$P$来估计概率密度函数$p$

假设n个样本$\mathbf{x}_1,\cdots,\mathbf{x}_n$，都是根据概率密度函数$p(\mathbf{x})$独立同分布(I.I.D.)抽取而得到的，$k$个样本落在区域$\mathcal{R}$中的概率服从二项式定理：

$$\displaystyle P_k=\left(\begin{array}{c}n\\k\end{array}\right)P^k(1-P)^{n-k}$$

$k$的期望为$nP$，比值$k/n$就是对$P$的一个很好的估计

假设$p(\mathbf{x})$是连续的，并且区域$\mathcal{R}$足够小，以至于在这个区间中$p$几乎无变化，那么

$$\displaystyle \int \limits_{\mathcal{R}}p(\mathbf{x}')d\mathbf{x}' = p(\mathbf{x})V$$

通过以上的推导可以得出对$p(\mathbf{x})$的估计：

$$\displaystyle p(\mathbf{x}) \approx \frac{k/n}{V}$$

为了估计点$\mathbf{x}$处的概率密度函数，构造一系列包含点$\mathbf{x}$的区域，$\mathcal{R}_1,\mathcal{R}_2,\cdots,\mathcal{R}_n$，$V_n$为区域$\mathcal{R}_n$的体积，$k_n$为落在区间$\mathcal{R}_n$中的样本个数，$p_n(\mathbf{x})$表示对$p(\mathbf{x})$的第$n$次估计：

$$\displaystyle p_n(\mathbf{x}) = \frac{k_n/n}{V_n}$$

$p_n(\mathbf{x})$收敛到$p(\mathbf{x})$的条件：

$$\lim \limits_{n \to \infty}V_n = 0$$

$$\lim \limits_{n \to \infty}k_n=\infty$$

$$\lim \limits_{n \to \infty}k_n/n=0$$

### [](#header-2)二、Parzen窗估计

Parzen窗估计是常用的概率密度函数非参数估计方法之一，根据某一个确定的体积函数，比如$V_n=1/\sqrt{n}$，逐渐收缩一个给定的初始区间，这就要求随机变量$k_n$和$k_n/n$能够保证$p_n(\mathbf{x})$收敛到$p(\mathbf{x})$

假设区间$\mathcal{R}_n$是一个以$\mathbf{x}$为中心的$d$维的超立方体，$h_n$表示超立方体一条边的长度，体积为$h_n^d$

定义窗函数：

$$\varphi(\pmb{\mu})=\begin{cases}1 & |\mu_j|\leq 1/2 ,j=1,\cdots,d\\0 & otherwise\end{cases}$$

该窗函数表示一个中心在原点的超立方体

如果$\mathbf{x}_i$落在超立方体$V_n$中，那么$\varphi\left((\mathbf{x}_i-\mathbf{x})/h_n\right)=1$，否则为0，则落在超立方体中的样本个数为：

$$\displaystyle k_n=\sum \limits_{i=1}^n \varphi \left(\frac{\mathbf{x}_i-\mathbf{x}}{h_n}\right)$$

那么

$$\displaystyle p_n(\mathbf{x})=\frac{1}{n}\sum \limits_{i=1}^n \frac{1}{V_n} \varphi \left(\frac{\mathbf{x}_i-\mathbf{x}}{h_n}\right)$$

对如上式子窗函数进行一定泛化，由此可以得到一般的估计概率密度方法，不必规定区间必须是超立方体，而可以是更一般化的形式，可以选取高斯函数为窗函数 

### [](#header-3)三、K-近邻估计

由于在Parzen窗估计中，最佳窗函数的选取总是一个问题，一种可行的解决方法就是让$k_n$为$n$的某个函数，而不是硬性的规定窗函数为全体样本个数的某个函数，比如$k_n=\sqrt{n}$，那么体积就必须逐渐生长，直到最后能包含进$\mathbf{x}$的$k_n$个相邻点

如果在点$\mathbf{x}$附近的概率密度比较小，那么这个体积就会比较小，如果$\mathbf{x}$附近的概率密度比较大，那么这个体积就会比较大，一旦进入某个概率密度很高的区域，这个体积的生长就会停止

$$\displaystyle p_n(\mathbf{x}) = \frac{k_n/n}{V_n}$$

$p_n(\mathbf{x})$收敛到$p(\mathbf{x})$的条件：

$$\lim \limits_{n \to \infty}k_n=\infty$$

$$\lim \limits_{n \to \infty}k_n/n=0$$

当$k_n=\sqrt{n}$时，$\displaystyle V_n=\frac{1}{\sqrt{n}p(\mathbf{x})}$，那么一维情况下：

$$\displaystyle p_n(x)=\frac{\sqrt{n}/n}{2|x-x_{knn}|}$$

### [](#header-4)四、K-近邻分类：后验概率

假设把一个体积放在$\mathbf{x}$周围，并且能够包含进$k$个样本，其中$k_i$个属于类别$\omega_i$，那么联合概率密度的估计显然是：

$$\displaystyle p_n(\mathbf{x},\omega_i) = \frac{k_i/n}{V_n}$$

$$\sum \limits_{i=1}^c k_i=k $$

$$\displaystyle p_n(\omega_i|\mathbf{x})=\frac{p_n(\mathbf{x},\omega_i)}{p_n(\mathbf{x})}$$

$$\displaystyle p_n(\mathbf{x})=\frac{k/n}{V_n}$$

那么

$$\displaystyle p_n(\omega_i|\mathbf{x})=\frac{k_i}{k}$$

点$\mathbf{x}$属于类别$\omega_i$的后验概率就是体积中标记为$\omega_i$的样本点的个数与体积中全部样本点个数的比值

为了达到最小的误差率，我们就选择使这个比值最大的那个类别作为判决结果

K-近邻分类的错误率趋近于贝叶斯错误率的条件：

$$\lim \limits_{n \to \infty}k_n=\infty$$

$$\lim \limits_{n \to \infty}k_n/n=0$$

K-近邻分类规则里面没有概率密度，但是该规则是从非参数概率密度估计和贝叶斯决策推导而来的

该文章已同步至[CSDN](http://blog.csdn.net/u013413471/article/)  
